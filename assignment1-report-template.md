# Assignment 1 Report

**Topic** - Introduction to Testing and Defect Tracking

## Table of Contents

-   [Introduction](#introduction)
-   [Video demo](#video-demo)
-   [Description of exploratory testing plan](#description-of-exploratory-testing-plan)
-   [Comparison of exploratory and scripted testing](#comparison-of-exploratory-and-scripted-testing)
-   [Notes and discussion of the peer reviews of defect reports](#notes-and-discussion-of-the-peer-reviews-of-defect-reports)
-   [Managing pair testing and division of team work](#managing-pair-testing-and-division-of-team-work)
-   [Difficulties, challenges, and lessons learned](#difficulties-challenges-and-lessons-learned)
-   [Comments and feedback](#comments-and-feedback)
-   [Contributors](#contributors)

## Introduction

In this lab work, we will be performing exploratory tests and scripted tests on the given ATM simulation system v1.0. It is available inside the zip archive [assignment1-artifacts.zip](assignment1-artifacts.zip), and the file name is **"ATM System - Lab 1 Version 1.0.jar"**.

If we find any bug during exploratory/scripted testing, we will log it in Backlog. Our Backlog is available at [https://seng637g5.backlog.com/dashboard](https://seng637g5.backlog.com/dashboard).

Then for each bug found, we will perform regression testing on the updated version of the given system, and update its status in the Backlog. The updated sytem is available inside the zip archive [assignment1-artifacts.zip](assignment1-artifacts.zip), and the file name is **"ATM System - Lab 1 Version 1.1.jar"**.

Before this lab, we didn't know the anything about exploratory tests and scripted tests and the differences between them. However, I (Bhavyai) had heard of Regression and regression testing from my previous job.

## Video demo

Make a video of the demo and put its link here.
All members must participate in the demo and the video should not be longer than 10 minutes.

## Description of exploratory testing plan

Below points summarizes the high level description of the exploratory testing plan that we followed among each pair.

#### Approach being taken

1. Explore the common routines that a user would do when he visits the ATM
2. Also explore what the user can potentially do when he visits the ATM, like pressing incorrect buttons
3. Explore all the functions little bit
4. After every transaction, check the balance in the accounts, and the logs to verify if everything has been recorded correctly
5. Check the receipt generated to see if it captures the correct details

#### Functionalities being targetted

1. Viewing balance of available accounts
2. Deposit of cash
3. Withdrawl of cash
4. Transfer of money
5. Verying the logs

## Comparison of exploratory and scripted testing

With exploratory testing our team has found many more bugs than during scripted testing sessions. We believe the reason for this is because each of our team members have found very innovative and diverse ways in operating the program. These diverse usages have allowed us to observe more bugs than the 40 scripted test cases. This is the biggest benefit of the exploratory testing, that we are not restricted during the testing. Also. exploratory testing is effective, especially when the scripted tests are not exhaustive. For example, the given forty manual scripted tests were simply not enough to cover most of the bugs.

<u>The trade-off with exploratory testing</u> - it was difficult to keep track our progress in testing out the programâ€™s functionality as nothing was planned out. Also, several bugs could possibly have been missed.

On the contrary, scripted testing is more efficient for keeping track of progress and for recording the bugs. The scripted tests can be easily followed by anyone to ensure that the code meets the minimum expected quality. Scripted tests, once written, do not rely on the ideas of the tester as they have to follow the exact steps during the testing.

<u>The trade-off with scripted testing</u> - it relies on the imagination of the writer of the tests. If a test case is left out, it will be left from all rounds of testing.

#### Bug report

1. The bug report generated by export of Backlog issues is made available [here](issues/Backlog_Export.xlsx).

## Notes and discussion of the peer reviews of defect reports

Text...

## Managing pair testing and division of team work

For exploratory testing, each of did individual tests. We recorded all of the bugs that were found in the process. We then collected every team member's recorded bugs and compiled it into a list. Finally, the bugs that were found were also tested in version 1.1 of the SUT.

For MFT and regression testing, we split the forty test cases evenly between two pairs; with cases 1-20 being done by Drew and Okeoghenemarho while cases 21-40 were completed by Bhavyai and Michael. In both the pairs, when one guy was testing, the other was recording the steps and bugs, and vice-versa. For each of the forty testcases, we first record what we observe in version 1.0 and then we record what we observed in version 1.1 of the SUT.

Finally, the two pairs would do a peer review of the MFT and regression testing. What this entails is to review the records for each cases in both version 1.0 and 1.1 to see if the records can be reproducible in the ATM program.

## Difficulties, challenges, and lessons learned

One of the difficulty that we faced was related to exploratory tests. Each pair of group reported similar set of bugs. So we had multiple bugs reported twice, that too with varied description of the same bug. Some of the bugs reported could not be reproduced.

Combining all those bugs to keep only the unique ones was a challenge. We overcame this by all coming together virtually over Discord, discussing what each pair has found, and then one person going over all issues to consolidate them in one document, while rest others helping in the process. We tested all the reported bugs during this process, and removed the bugs that couldn't be reproduced at all.

The lesson learned from this was team work. The whole team should be on the same page. They must know what's should be done and how other team members are contributing to the same assignment. We must maintain a central document or place where each team member should look if an issue has already been reported before adding his/her own issue. This would reduce duplicacy and unnecesary efforts to remove that duplicacy.

## Comments and feedback

1. Backlog was used as a tool for reporting and managing bugs. In our opinion, it was perfect for this assignment. Now we are familiar with three issue tracking tools - other two being Jira and GitHub.

2. The SUT chosen for this assignment was a great example for both exploratory testing/scripted testing and regression testing. It was simple enough to be understood by anyone, yet it had plenty of bugs to be discovered.

3. The assignment description document [`assignment1.md`](assignment1.md) is very detailed and comprehensive, and it was easy to follow.

## Contributors

We are group 5, and below are the team members

-   [Bhavyai Gupta](https://github.com/zbhavyai)
-   [Drew Burritt](https://github.com/dburritt)
-   [Michael Man Yin Lee](https://github.com/mlee2021)
-   [Okeoghenemarho Obuareghe](https://github.com/oobuareghe)
